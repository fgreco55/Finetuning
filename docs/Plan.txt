OVERALL PLAN
------------
Assumptions: english text, OpenAI GPT, Theo Fanning's Java API to OpenAI, Milvus Vector DB

1. Identify a docset.  This docset could be TXT, PDF, HTML, etc.
2. If it isn't already, convert this docset into simple text. Optionally remove noise words to help with vector search in another step.
   Start with a file.  Enhancement would be to allow multiple files or even folders or URLs
3. Parse this docset into sentences (we'll use the granularity of sentences as our 'knowledge')
4. Create embeddings for each sentence in the docset.  Use the embeddings service from OpenAI.
5. Save the embeddings in the vector DB

Application

6. Get prompt from the user, ie, the "user-prompt"
7. Create the embedding of this prompt using OpenAI.
8. Use the semantic search capabilities of the vector DB for the text of the closest matches.  We'll use the top-10 matches.  This will be part of our prompt "preamble"
9. Construct a new prompt [follow OpenAI prompt suggestions]:
    Context + text-of-the-top-10-closest-matches + user-prompt
    Context is how you want the model to respond.  See: https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api
    "super" prompt suggestion: "Answer the following query: {original query text} with the given context: {text chunks, sentences}"
10. Send to OpenAI GPT using the appropriate model.
11. Display result

Test results.
When results are not correct, we need to tweak the docset (step 1) and repeat everything.

Useful target CLI app would be:
     % pchat [-a assistant-preamble] [file(s)] [directory(s)] [URL(s)]
    Should design this as a server with a CLI/web/app front-end
    A pchat server would be cool.  The CLI would just send commands to the server.  Then you could converse with the LLM (assistant)
      and add more "knowledge" dynamically.  And it implies you can easily have multiple instances.
    Eventually add a config file that identifies information sources, API keys, assistant preambles
    Need a "DataLoader" class that encapsulates VectorDB

Useful preamble:
   You are a helpful assistant for the NYJavaSIG.
   Only answer questions about the NYJavaSIG and no other topic.
   Do not answer any questions about any other topic.  Be polite and very helpful.

Thoughts:
   Create a PChat java class that encapsulates LLM.java and VectorDB.java
        or a DataLoader class that encapsulates VectorDB.java and a PChat class that has a Loader object
   Create a phatd server that surrounds PChat.java and accepts commands, eg, loadfile, send, set model, set context, etc
     Then you can have several instances running... each potentially connected to different LLMs.
   Create a "pchat" cli that connects to phatd for shell-level use.
   Once you had separate phatd servers, then you can host info servers per user
   Separate Collections per user in one Vector DB [Bhupendra]     - need to investigate privacy/security aspects